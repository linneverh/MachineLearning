{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/MGT-502-Data-Science-and-Machine-Learning/blob/main/12_Chatbots/Chatgpt_prompt_engineering_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMUz953G6AWx"
      },
      "source": [
        "# Getting Started with Prompt Engineering\n",
        "\n",
        "(credits DAIR.AI)\n",
        "\n",
        "\n",
        "This notebook contains examples and exercises to learning about prompt engineering.\n",
        "\n",
        "We will be using the [OpenAI APIs](https://platform.openai.com/) for all examples. I am using the default settings `temperature=0.7` and `top-p=1`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8JaNENL6AW0"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qFsyMSq6AW1"
      },
      "source": [
        "## 1. Prompt Engineering Basics\n",
        "\n",
        "Objectives\n",
        "- Load the libraries\n",
        "- Review the format\n",
        "- Cover basic prompts\n",
        "- Review common use cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj7saJcV6AW1"
      },
      "source": [
        "Below we are loading the necessary libraries, utilities, and configurations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBeKoMxO6AW3",
        "outputId": "a16e73fc-de16-489d-9733-2086bf75db52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# update or install the necessary libraries\n",
        "!pip install openai==0.28\n",
        "!pip install -q --upgrade langchain\n",
        "!pip install -q --upgrade python-dotenv\n",
        "!pip install -q chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FA8kBfI6AW3"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import IPython\n",
        "from langchain.llms import OpenAI\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhRFKk6K6AW3"
      },
      "source": [
        "Load environment variables. You can use anything you like but I used `python-dotenv`. Just create a `.env` file with your `OPENAI_API_KEY` then load it.\n",
        "\n",
        "- Go to https://serper.dev/ to get the API key to do google searches if you want to execute some of the cells down below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env OPENAI_API_KEY="
      ],
      "metadata": {
        "id": "TRyMPPIY9ZUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3006933-3464-465f-9883-44df25e1a174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: OPENAI_API_KEY=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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%env SERPAPI_API_KEY="
      ],
      "metadata": {
        "id": "TFcPG1bd-pR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f49ba7-b988-447e-d679-2c558b828758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: SERPAPI_API_KEY=bd6ee2c07fb32766e877564c536131abb2da4174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWJyNZhM6AW4"
      },
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "\n",
        "# API configuration\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# for LangChain\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
        "os.environ[\"SERPAPI_API_KEY\"] = os.getenv(\"SERPAPI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87PLz-XD6AW4"
      },
      "outputs": [],
      "source": [
        "def set_open_params(\n",
        "    model=\"gpt-3.5-turbo-instruct\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "):\n",
        "    \"\"\" set openai parameters\"\"\"\n",
        "\n",
        "    openai_params = {}\n",
        "\n",
        "    openai_params['model'] = model\n",
        "    openai_params['temperature'] = temperature\n",
        "    openai_params['max_tokens'] = max_tokens\n",
        "    openai_params['top_p'] = top_p\n",
        "    openai_params['frequency_penalty'] = frequency_penalty\n",
        "    openai_params['presence_penalty'] = presence_penalty\n",
        "    return openai_params\n",
        "\n",
        "def get_completion(params, prompt):\n",
        "    \"\"\" GET completion from openai api\"\"\"\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "        engine = params['model'],\n",
        "        prompt = prompt,\n",
        "        temperature = params['temperature'],\n",
        "        max_tokens = params['max_tokens'],\n",
        "        top_p = params['top_p'],\n",
        "        frequency_penalty = params['frequency_penalty'],\n",
        "        presence_penalty = params['presence_penalty'],\n",
        "    )\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W34SqmlF6AW4"
      },
      "source": [
        "Basic prompt example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvGyOtfF6AW4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "65b85893-c439-4db9-bbff-0d32481c5dc8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-9b7bdf8fafde>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The sky is\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-8e72c739c2ed>\u001b[0m in \u001b[0;36mget_completion\u001b[0;34m(params, prompt)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;34m\"\"\" GET completion from openai api\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     response = openai.Completion.create(\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
          ]
        }
      ],
      "source": [
        "# basic example\n",
        "params = set_open_params()\n",
        "\n",
        "prompt = \"The sky is\"\n",
        "\n",
        "response = get_completion(params, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RDbXIzN6AW4",
        "outputId": "0b53792f-d576-4d48-aee0-a0c861419910"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" blue\\n\\nThe sky is blue because of the way the Earth's atmosphere scatters sunlight. The blue color is caused by the shorter wavelengths of blue light being scattered more effectively by the atmosphere than the longer wavelengths of other colors.\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.choices[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xyQLcfJ6AW5",
        "outputId": "3c0e97e8-6106-4f88-a256-1591717fcf87"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " blue\n",
              "\n",
              "The sky is blue because of the way the Earth's atmosphere scatters sunlight. The blue color is caused by the shorter wavelengths of blue light being scattered more effectively by the atmosphere than the longer wavelengths of other colors."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "IPython.display.Markdown(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbTevKRs6AW5"
      },
      "source": [
        "Try with different temperature to compare results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54ZvNxjZ6AW5",
        "outputId": "d4bad3ea-0935-4820-9718-45745684a5bd"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " blue\n",
              "\n",
              "The sky is blue because of the way the atmosphere scatters sunlight. When sunlight passes through the atmosphere, the blue wavelengths are scattered more than the other colors, making the sky appear blue."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "params = set_open_params(temperature=0)\n",
        "prompt = \"The sky is\"\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awtBylkh6AW5"
      },
      "source": [
        "### 1.1 Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEzEkcOr6AW6",
        "outputId": "6dde36b5-3b16-4c24-cbdb-5c231a4185b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\nAntibiotics are medications used to treat bacterial infections by either killing the bacteria or preventing them from reproducing, but they cannot be used to treat viral infections and can lead to antibiotic resistance if used incorrectly."
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "params = set_open_params(temperature=0.7)\n",
        "prompt = \"\"\"Antibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body's immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance.\n",
        "\n",
        "Explain the above in one sentence:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUFw7piF6AW6"
      },
      "source": [
        "Exercise: Instruct the model to explain the paragraph in one sentence like \"I am 5\". Do you see any differences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqJt0wby6AW6"
      },
      "source": [
        "### 1.2 Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ORPV8of6AW6",
        "outputId": "1b25afee-da87-4ca1-f2e5-105956a11b20"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " Mice"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"Answer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.\n",
        "\n",
        "Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.\n",
        "\n",
        "Question: What was OKT3 originally sourced from?\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJRMT_cH6AW6"
      },
      "source": [
        "Context obtained from here: https://www.nature.com/articles/d41586-023-00400-x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1cCOBlK6AW6"
      },
      "source": [
        "Exercise: Edit prompt and get the model to respond that it isn't sure about the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6eC4B5G6AW6"
      },
      "source": [
        "### 1.3 Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AWUcxsE6AW7",
        "outputId": "8d89f465-a4e0-44cf-e95e-c3a5adf610ee"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " Neutral"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"Classify the text into neutral, negative or positive.\n",
        "\n",
        "Text: I think the food was okay.\n",
        "\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcCkcg9j6AW7"
      },
      "source": [
        "Exercise: Modify the prompt to instruct the model to provide an explanation to the answer selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0x83Uqq6AW7"
      },
      "source": [
        "### 1.4 Role Playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKJ5sNfJ6AW7",
        "outputId": "fb3093a2-f505-4311-a664-a84955d132a4"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " Of course! Black holes are formed when a massive star (at least 3 times the mass of the Sun) runs out of fuel and its core collapses, resulting in a gravitational pull so strong that nothing, not even light, can escape. This collapse is accompanied by a massive burst of energy, which can be detected by astronomers."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"The following is a conversation with an AI research assistant. The assistant tone is technical and scientific.\n",
        "\n",
        "Human: Hello, who are you?\n",
        "AI: Greeting! I am an AI research assistant. How can I help you today?\n",
        "Human: Can you tell me about the creation of blackholes?\n",
        "AI:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojWcwaYg6AW7"
      },
      "source": [
        "Exercise: Modify the prompt to instruct the model to keep AI responses concise and short."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5vjen016AW7"
      },
      "source": [
        "### 1.5 Code Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrkbU0UJ6AW7",
        "outputId": "36a7a436-c7d2-4703-d90e-52384f470df5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "\n",
              "SELECT StudentId, StudentName \n",
              "FROM students \n",
              "WHERE DepartmentId = (SELECT DepartmentId \n",
              "                      FROM departments \n",
              "                      WHERE DepartmentName = 'Computer Science');"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\\\"\\\"\\\"\\nTable departments, columns = [DepartmentId, DepartmentName]\\nTable students, columns = [DepartmentId, StudentId, StudentName]\\nCreate a MySQL query for all students in the Computer Science Department\\n\\\"\\\"\\\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myHAmbOq6AW8"
      },
      "source": [
        "### 1.6 Reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiBGZ1006AW9",
        "outputId": "71b68c72-cf61-4128-f686-25d7fde8f53e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " \n",
              "\n",
              "Odd numbers: 15, 5, 13, 7, 1\n",
              "Sum of odd numbers: 41\n",
              "Result: 41 is an odd number"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "\n",
        "Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A0FE4Ts6AW9"
      },
      "source": [
        "Exercise: Improve the prompt to have a better structure and output format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jkt8SKV6AW-"
      },
      "source": [
        "## 2. Advanced Prompting Techniques\n",
        "\n",
        "Objectives:\n",
        "\n",
        "- Cover more advanced techniques for prompting: few-shot, chain-of-thoughts,..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Jl6dAt6AW-"
      },
      "source": [
        "### 2.2 Few-shot prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFbdQiAf6AW-",
        "outputId": "1a14dac5-4ab7-4b0f-a580-e99d9878f971"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " The answer is True."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
        "A: The answer is True.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
        "A: The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_wPc9lx6AW_"
      },
      "source": [
        "### 2.3 Chain-of-Thought (CoT) Prompting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk_-InYO6AW_",
        "outputId": "6621cfa1-684d-42d2-b4f2-26beb144807a"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
        "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
        "\n",
        "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\n",
        "A:\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXPdAopk6AXA"
      },
      "source": [
        "### 2.4 Zero-shot CoT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jULFj1tg6AXA",
        "outputId": "bec06d2e-3293-4436-849b-a8ce70c4903f"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              " \n",
              "\n",
              "Initially, you had 10 apples. \n",
              "\n",
              "Then, you gave away 2 apples to the neighbor and 2 apples to the repairman. That's 4 apples. \n",
              "\n",
              "So now, you have 6 apples left. \n",
              "\n",
              "Then, you went and bought 5 more apples. That's 11 apples now. \n",
              "\n",
              "Finally, you ate 1 apple. \n",
              "\n",
              "That means you remain with 10 apples."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "\n",
        "Let's think step by step.\"\"\"\n",
        "\n",
        "response = get_completion(params, prompt)\n",
        "IPython.display.Markdown(response.choices[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYwbl3FW6AXA"
      },
      "source": [
        "### 2.6 PAL - Code as Reasoning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDlIMEEK6AXA"
      },
      "source": [
        "We are developing a simple application that's able to reason about the question being asked through code.\n",
        "\n",
        "Specifically, the application takes in some data and answers a question about the data input. The prompt includes a few exemplars which are adopted from [here](https://github.com/reasoning-machines/pal/blob/main/pal/prompt/penguin_prompt.py).  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAhKREr56AXB"
      },
      "outputs": [],
      "source": [
        "# lm instance\n",
        "llm = OpenAI(model_name='text-davinci-003', temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTv-ANlD6AXB"
      },
      "outputs": [],
      "source": [
        "question = \"Which is the oldest penguin?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrKYKNMU6AXB"
      },
      "outputs": [],
      "source": [
        "PENGUIN_PROMPT = '''\n",
        "\"\"\"\n",
        "Q: Here is a table where the first line is a header and each subsequent line is a penguin:\n",
        "name, age, height (cm), weight (kg)\n",
        "Louis, 7, 50, 11\n",
        "Bernard, 5, 80, 13\n",
        "Vincent, 9, 60, 11\n",
        "Gwen, 8, 70, 15\n",
        "For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.\n",
        "We now add a penguin to the table:\n",
        "James, 12, 90, 12\n",
        "How many penguins are less than 8 years old?\n",
        "\"\"\"\n",
        "# Put the penguins into a list.\n",
        "penguins = []\n",
        "penguins.append(('Louis', 7, 50, 11))\n",
        "penguins.append(('Bernard', 5, 80, 13))\n",
        "penguins.append(('Vincent', 9, 60, 11))\n",
        "penguins.append(('Gwen', 8, 70, 15))\n",
        "# Add penguin James.\n",
        "penguins.append(('James', 12, 90, 12))\n",
        "# Find penguins under 8 years old.\n",
        "penguins_under_8_years_old = [penguin for penguin in penguins if penguin[1] < 8]\n",
        "# Count number of penguins under 8.\n",
        "num_penguin_under_8 = len(penguins_under_8_years_old)\n",
        "answer = num_penguin_under_8\n",
        "\"\"\"\n",
        "Q: Here is a table where the first line is a header and each subsequent line is a penguin:\n",
        "name, age, height (cm), weight (kg)\n",
        "Louis, 7, 50, 11\n",
        "Bernard, 5, 80, 13\n",
        "Vincent, 9, 60, 11\n",
        "Gwen, 8, 70, 15\n",
        "For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.\n",
        "Which is the youngest penguin?\n",
        "\"\"\"\n",
        "# Put the penguins into a list.\n",
        "penguins = []\n",
        "penguins.append(('Louis', 7, 50, 11))\n",
        "penguins.append(('Bernard', 5, 80, 13))\n",
        "penguins.append(('Vincent', 9, 60, 11))\n",
        "penguins.append(('Gwen', 8, 70, 15))\n",
        "# Sort the penguins by age.\n",
        "penguins = sorted(penguins, key=lambda x: x[1])\n",
        "# Get the youngest penguin's name.\n",
        "youngest_penguin_name = penguins[0][0]\n",
        "answer = youngest_penguin_name\n",
        "\"\"\"\n",
        "Q: Here is a table where the first line is a header and each subsequent line is a penguin:\n",
        "name, age, height (cm), weight (kg)\n",
        "Louis, 7, 50, 11\n",
        "Bernard, 5, 80, 13\n",
        "Vincent, 9, 60, 11\n",
        "Gwen, 8, 70, 15\n",
        "For example: the age of Louis is 7, the weight of Gwen is 15 kg, the height of Bernard is 80 cm.\n",
        "What is the name of the second penguin sorted by alphabetic order?\n",
        "\"\"\"\n",
        "# Put the penguins into a list.\n",
        "penguins = []\n",
        "penguins.append(('Louis', 7, 50, 11))\n",
        "penguins.append(('Bernard', 5, 80, 13))\n",
        "penguins.append(('Vincent', 9, 60, 11))\n",
        "penguins.append(('Gwen', 8, 70, 15))\n",
        "# Sort penguins by alphabetic order.\n",
        "penguins_alphabetic = sorted(penguins, key=lambda x: x[0])\n",
        "# Get the second penguin sorted by alphabetic order.\n",
        "second_penguin_name = penguins_alphabetic[1][0]\n",
        "answer = second_penguin_name\n",
        "\"\"\"\n",
        "{question}\n",
        "\"\"\"\n",
        "'''.strip() + '\\n'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stM9lt1w6AXC"
      },
      "source": [
        "Now that we have the prompt and question. We can send it to the model. It should output the steps, in code, needed to get the solution to the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoGHStFc6AXC",
        "outputId": "a3be02db-85c9-49aa-d911-d3cf8cbeb435"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Put the penguins into a list.\n",
            "penguins = []\n",
            "penguins.append(('Louis', 7, 50, 11))\n",
            "penguins.append(('Bernard', 5, 80, 13))\n",
            "penguins.append(('Vincent', 9, 60, 11))\n",
            "penguins.append(('Gwen', 8, 70, 15))\n",
            "# Sort the penguins by age.\n",
            "penguins = sorted(penguins, key=lambda x: x[1], reverse=True)\n",
            "# Get the oldest penguin's name.\n",
            "oldest_penguin_name = penguins[0][0]\n",
            "answer = oldest_penguin_name\n"
          ]
        }
      ],
      "source": [
        "llm_out = llm(PENGUIN_PROMPT.format(question=question))\n",
        "print(llm_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOBhGatW6AXC",
        "outputId": "7b9661c9-3b8c-4a1b-82f5-dddfaa40e31d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vincent\n"
          ]
        }
      ],
      "source": [
        "exec(llm_out)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI6BBbbf6AXC"
      },
      "source": [
        "That's the correct answer! Vincent is the oldest penguin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYyzk7uy6AXC"
      },
      "source": [
        "Exercise: Try a different question and see what's the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JxBDwX46AXC"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILKthOia6AXC"
      },
      "source": [
        "# 3. Tools and Applications\n",
        "\n",
        "Objective:\n",
        "\n",
        "- Demonstrate how to use LangChain to demonstrate simple applications using prompting techniques and LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lreDljrM6AXD"
      },
      "source": [
        "### 3.2 Data-Augmented Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17zcMtmJ6AXD"
      },
      "source": [
        "First, we need to download the data we want to use as source to augment generation.\n",
        "\n",
        "Code example adopted from [LangChain Documentation](https://langchain.readthedocs.io/en/latest/modules/chains/combine_docs_examples/qa_with_sources.html). We are only using the examples for educational purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh0_R-FH6AXD"
      },
      "source": [
        "Prepare the data first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIRqb2IM6AXD"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings.cohere import CohereEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores.elastic_vector_search import ElasticVectorSearch\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygKPfCC36AXD"
      },
      "outputs": [],
      "source": [
        "with open('./state_of_the_union.txt') as f:\n",
        "    state_of_the_union = f.read()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_text(state_of_the_union)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeJJ7FBY6AXD"
      },
      "outputs": [],
      "source": [
        "import chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQmlozFq6AXD"
      },
      "outputs": [],
      "source": [
        "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs7t_qCj6AXD"
      },
      "outputs": [],
      "source": [
        "query = \"What did the president say about Justice Breyer\"\n",
        "docs = docsearch.similarity_search(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCd8HhnF6AXD"
      },
      "source": [
        "Let's quickly test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08LKL62V6AXD"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD8VOYjF6AXD",
        "outputId": "5e53c1b9-ef39-4744-b347-22b6c9b47b36"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'output_text': ' The president thanked Justice Breyer for his service.\\nSOURCES: 30-pl'}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\")\n",
        "query = \"What did the president say about Justice Breyer\"\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h50NYubK6AXD"
      },
      "source": [
        "Let's try a question with a custom prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viR-IU3n6AXE",
        "outputId": "dbaaf582-0394-4469-f588-071c9bc5a7ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'output_text': '\\nEl Presidente no dijo nada acerca de la Justicia Breyer.\\n\\nFUENTES: 30, 31, 33'}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "template = \"\"\"Given the following extracted parts of a long document and a question, create a final answer with references (\"SOURCES\").\n",
        "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
        "ALWAYS return a \"SOURCES\" part in your answer.\n",
        "Respond in Spanish.\n",
        "\n",
        "QUESTION: {question}\n",
        "=========\n",
        "{summaries}\n",
        "=========\n",
        "FINAL ANSWER IN SPANISH:\"\"\"\n",
        "\n",
        "# create a prompt template\n",
        "PROMPT = PromptTemplate(template=template, input_variables=[\"summaries\", \"question\"])\n",
        "\n",
        "# query\n",
        "chain = load_qa_with_sources_chain(OpenAI(temperature=0), chain_type=\"stuff\", prompt=PROMPT)\n",
        "query = \"What did the president say about Justice Breyer?\"\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUtCORdk6AXE"
      },
      "source": [
        "Exercise: Try using a different dataset from the internet and try different prompt, including all the techniques you learned in the lecture."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "promptlecture",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "f38e0373277d6f71ee44ee8fea5f1d408ad6999fda15d538a69a99a1665a839d"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}